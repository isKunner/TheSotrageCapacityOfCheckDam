#!/usr/bin/env python
# -*- coding:utf-8 -*-
# @FileName: USGS_download
# @Time    : 2026/1/26 21:42
# @Author  : Kevin
# @Describe:
import json
import os.path
import re
from math import ceil
from urllib.parse import urlparse

import rasterio
import requests
import utm
import geopandas as gpd
from shapely.geometry import Point

import concurrent.futures
from tqdm import tqdm

from LocalPath import dam_google_remote_root_path, dam_usgs_dem_index_root_path, USA_States, dam_usgs_dem_root_path

state_to_abbr = {
    "alabama": "al",
    "alaska": "ak",
    "arizona": "az",
    "arkansas": "ar",
    "california": "ca",
    "colorado": "co",
    "connecticut": "ct",
    "delaware": "de",
    "florida": "fl",
    "georgia": "ga",
    "hawaii": "hi",
    "idaho": "id",
    "illinois": "il",
    "indiana": "in",
    "iowa": "ia",
    "kansas": "ks",
    "kentucky": "ky",
    "louisiana": "la",
    "maine": "me",
    "maryland": "md",
    "massachusetts": "ma",
    "michigan": "mi",
    "minnesota": "mn",
    "mississippi": "ms",
    "missouri": "mo",
    "montana": "mt",
    "nebraska": "ne",
    "nevada": "nv",
    "new hampshire": "nh",
    "new jersey": "nj",
    "new mexico": "nm",
    "new york": "ny",
    "north carolina": "nc",
    "north dakota": "nd",
    "ohio": "oh",
    "oklahoma": "ok",
    "oregon": "or",
    "pennsylvania": "pa",
    "rhode island": "ri",
    "south carolina": "sc",
    "south dakota": "sd",
    "tennessee": "tn",
    "texas": "tx",
    "utah": "ut",
    "vermont": "vt",
    "virginia": "va",
    "washington": "wa",
    "west virginia": "wv",
    "wisconsin": "wi",
    "wyoming": "wy"
}



def download_single_file(url, download_dir):
    """
    ä¸‹è½½å•ä¸ªæ–‡ä»¶
    """
    try:
        filename = os.path.basename(urlparse(url).path)
        filepath = os.path.join(download_dir, filename)

        if os.path.exists(filepath):
            return True

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        response = requests.get(url, stream=True, timeout=30, headers=headers)
        response.raise_for_status()

        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        return True
    except Exception as e:
        print(f"Error downloading file: {e}")
        return False


def get_file_name_part(lon, lat):
    easting, northing, zone_number, zone_letter = utm.from_latlon(lat, lon)
    return f"x{int(easting//10000)}y{ceil(northing/10000)}"

def get_tif_bounds(google_file_path):
    """
    è¯»å–TIFæ–‡ä»¶å¹¶è·å–å››ä¸ªè§’åæ ‡ç‚¹
    è¿”å›æ ¼å¼: (å·¦ä¸‹, å³ä¸‹, å³ä¸Š, å·¦ä¸Š)
    """
    with rasterio.open(google_file_path) as dataset:
        # è·å–TIFæ–‡ä»¶çš„è¾¹ç•Œä¿¡æ¯
        bounds = dataset.bounds  # (left, bottom, right, top)
        crs = dataset.crs

        left, bottom, right, top = bounds

        # å››ä¸ªè§’ç‚¹åæ ‡
        lower_left = left, bottom  # å·¦ä¸‹è§’
        lower_right = right, bottom  # å³ä¸‹è§’
        upper_right = right, top  # å³ä¸Šè§’
        upper_left = left, top  # å·¦ä¸Šè§’

        return [lower_left, lower_right, upper_right, upper_left]


def down_load_file(dam_google_remote_root_dir, index_dict, usa_states_gdf, down_dict_info, group_name):
    """
    æ”¹è¿›ç‰ˆï¼šè®°å½•æ‰€æœ‰æ–‡ä»¶ï¼ŒåŒ…æ‹¬å¤±è´¥çš„ï¼Œå¹¶æ‰“å°è¯¦ç»†è°ƒè¯•ä¿¡æ¯
    """
    import re
    import math

    # è·å–æ‰€æœ‰tifæ–‡ä»¶å¹¶æ’åº
    all_files = [f for f in os.listdir(dam_google_remote_root_dir) if f.endswith(".tif")]
    all_files.sort(key=lambda x: int(re.match(r'(\d+)\.tif', x).group(1)) if re.match(r'(\d+)\.tif', x) else 0)

    for google_remote_file in tqdm(all_files, desc=f"å¤„ç† {group_name}"):
        # å¼ºåˆ¶åˆå§‹åŒ–è®°å½•
        if group_name not in down_dict_info:
            down_dict_info[group_name] = {}
        down_dict_info[group_name][google_remote_file] = {}

        try:
            # 1. è·å–å››è§’åæ ‡
            tif_path = os.path.join(dam_google_remote_root_dir, google_remote_file)
            coordinates = get_tif_bounds(tif_path)
            coordinates_set = set()

            # print(f"\nã€å¤„ç†ã€‘{google_remote_file}")
            # print(f"  å››è§’åæ ‡: {coordinates}")

            # 2. åˆ¤æ–­å·å’Œè®¡ç®—file_name_part
            for i, (lon, lat) in enumerate(coordinates):
                point = Point(lon, lat)
                matched = False

                for idx, state_row in usa_states_gdf.iterrows():
                    state_name = str(state_row['NAME']).lower().strip()

                    if state_row['geometry'].contains(point):
                        state_abbr = state_to_abbr.get(state_name)
                        if state_abbr:
                            file_part = get_file_name_part(lon, lat)
                            coordinates_set.add((state_abbr.upper(), file_part))
                            matched = True
                            # print(
                            #     f"  è§’ç‚¹{i + 1} ({lon:.6f}, {lat:.6f}): å·={state_abbr.upper()}, file_part={file_part}")
                            break
                        else:
                            print(f"  âš ï¸ è­¦å‘Š: å· '{state_row['NAME']}' æ— ç¼©å†™æ˜ å°„")

                if not matched:
                    print(f"  âŒ è§’ç‚¹{i + 1} ({lon:.6f}, {lat:.6f}): æœªåŒ¹é…åˆ°ä»»ä½•å·")

            # 3. æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°å·
            if not coordinates_set:
                down_dict_info[group_name][google_remote_file]["__error__"] = "no_state_found"
                down_dict_info[group_name][google_remote_file]["__detail__"] = f"å››è§’åæ ‡: {coordinates}"
                print(f"  ã€å¤±è´¥ã€‘æœªåŒ¹é…åˆ°ä»»ä½•å·")
                continue

            # 4. åœ¨ç´¢å¼•ä¸­æŸ¥æ‰¾é“¾æ¥ï¼ˆå…³é”®æ”¹è¿›ï¼šæ‰“å°è¯¦ç»†åŒ¹é…ä¿¡æ¯ï¼‰
            link_found = False
            for state, file_name_part in coordinates_set:
                # print(f"\n  ã€æŸ¥æ‰¾ç´¢å¼•ã€‘å·={state}, å¯»æ‰¾ file_part='{file_name_part}'")

                if state not in index_dict:
                    down_dict_info[group_name][google_remote_file]["__error__"] = f"state_not_in_index:{state}"
                    print(f"    âŒ ç´¢å¼•å­—å…¸ä¸­æ— å· '{state}'")
                    print(f"    å¯ç”¨å·åˆ—è¡¨: {list(index_dict.keys())[:10]}...")  # æ‰“å°å‰10ä¸ª
                    continue

                # æ‰“å°ç´¢å¼•æ ·æœ¬ï¼ˆå‰3ä¸ªå’Œå3ä¸ªï¼Œçœ‹å‘½åæ ¼å¼ï¼‰
                idx_list = index_dict[state]
                # print(f"    ç´¢å¼•æ–‡ä»¶æ•°: {len(idx_list)}")
                # print(f"    ç´¢å¼•æ ·æœ¬(å‰3): {idx_list[:3]}")
                # if len(idx_list) > 6:
                #     print(f"    ç´¢å¼•æ ·æœ¬(å3): {idx_list[-3:]}")

                # å°è¯•åŒ¹é…
                matched_links = []
                for link in idx_list:
                    if file_name_part in link:
                        matched_links.append(link)
                        down_dict_info[group_name][google_remote_file][link] = True
                        link_found = True
                        # print(f"    âœ“ åŒ¹é…: {link}")

                if not matched_links:
                    print(f"    âŒ æœªæ‰¾åˆ°åŒ…å« '{file_name_part}' çš„é“¾æ¥")
                    # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæ‰¾ç›¸ä¼¼çš„éƒ¨åˆ†
                    similar = [l for l in idx_list if file_name_part[:5] in l or file_name_part[-5:] in l][:3]
                    if similar:
                        print(f"    ç›¸ä¼¼é“¾æ¥(ä¾›å‚è€ƒ): {similar}")

            # 5. æœ€ç»ˆå¤±è´¥è®°å½•
            if not link_found:
                down_dict_info[group_name][google_remote_file]["__error__"] = "no_link_found"
                down_dict_info[group_name][google_remote_file]["__detail__"] = {
                    "states_searched": [c[0] for c in coordinates_set],
                    "file_parts": [c[1] for c in coordinates_set],
                    "coordinates": coordinates
                }
                print(f"\n  ã€æœ€ç»ˆå¤±è´¥ã€‘æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…é“¾æ¥")

        except Exception as e:
            import traceback
            error_msg = f"{str(e)}\n{traceback.format_exc()}"
            down_dict_info[group_name][google_remote_file]["__error__"] = "exception"
            down_dict_info[group_name][google_remote_file]["__detail__"] = error_msg
            print(f'\n  ã€å¼‚å¸¸ã€‘{google_remote_file}: {e}')

        # æ¯10ä¸ªä¿å­˜ä¸€æ¬¡
        if len(down_dict_info[group_name]) % 10 == 0:
            with open(os.path.join(dam_usgs_dem_root_path, "DownloadInfo.json"), 'w', encoding='utf-8') as f:
                json.dump(down_dict_info, f, ensure_ascii=False, indent=2)

    # æœ€ç»ˆä¿å­˜
    with open(os.path.join(dam_usgs_dem_root_path, "DownloadInfo.json"), 'w', encoding='utf-8') as f:
        json.dump(down_dict_info, f, ensure_ascii=False, indent=2)

    # ç»Ÿè®¡
    total = len(down_dict_info.get(group_name, {}))
    success = sum(1 for f, links in down_dict_info.get(group_name, {}).items()
                  if not any(k.startswith("__") for k in links.keys()))
    failed = total - success
    print(f"\n{'=' * 60}")
    print(f"[{group_name}] å®Œæˆ: æ€»è®¡{total}, æˆåŠŸ{success}, å¤±è´¥{failed}")
    print(f"{'=' * 60}")


def down_load_file_v2(dam_google_remote_root_dir, index_dict, usa_states_gdf, output_dir, down_dict_info, group_name):
    """
    æ”¹è¿›ç‰ˆï¼šå…ˆæŸ¥æœ¬åœ°å·ï¼Œæœ¬åœ°æ‰¾ä¸åˆ°å†æ‰©å¤§èŒƒå›´æŸ¥ä¸´è¿‘å·
    """
    all_files = [f for f in os.listdir(dam_google_remote_root_dir) if f.endswith(".tif")]
    all_files.sort(key=lambda x: int(re.match(r'(\d+)\.tif', x).group(1)) if re.match(r'(\d+)\.tif', x) else 0)

    for google_remote_file in tqdm(all_files, desc=f"å¤„ç† {group_name}"):
        if group_name not in down_dict_info:
            down_dict_info[group_name] = {}
        down_dict_info[group_name][google_remote_file] = {}

        try:
            tif_path = os.path.join(dam_google_remote_root_dir, google_remote_file)
            coordinates = get_tif_bounds(tif_path)

            print(f"\nã€å¤„ç†ã€‘{google_remote_file}")
            print(f"  å››è§’: {[(round(lon, 4), round(lat, 4)) for lon, lat in coordinates]}")

            # ç¬¬ä¸€æ­¥ï¼šåˆ†ç¦»æœ¬åœ°å·å’Œä¸´è¿‘å·
            local_candidates = []  # ä¸¥æ ¼åŒ…å«çš„å·ï¼ˆdistance=0ï¼‰
            nearby_candidates = []  # ä¸´è¿‘å·ï¼ˆdistance>0ä½†åœ¨bufferå†…ï¼‰

            for lon, lat in coordinates:
                point = Point(lon, lat)
                file_part = get_file_name_part(lon, lat)

                for idx, row in usa_states_gdf.iterrows():
                    state_name = str(row['NAME']).strip().lower()
                    geom = row['geometry']
                    abbr = state_to_abbr.get(state_name)
                    if not abbr:
                        continue

                    distance = geom.distance(point)

                    if geom.contains(point):
                        # ä¸¥æ ¼åŒ…å«ï¼šæœ¬åœ°å·
                        local_candidates.append((abbr.upper(), file_part, 0))
                        print(f"  ğŸ  æœ¬åœ°å·: {abbr.upper()}, file_part={file_part}")
                    elif distance < 1:  # 1åº¦â‰ˆ100kmèŒƒå›´å†…
                        # ä¸´è¿‘å·
                        nearby_candidates.append((abbr.upper(), file_part, distance))
                        print(f"  ğŸ“ ä¸´è¿‘å·: {abbr.upper()} (è·ç¦»{distance:.4f}Â°), file_part={file_part}")

            # å»é‡
            seen_local = set()
            unique_local = []
            for c in local_candidates:
                if c[:2] not in seen_local:
                    seen_local.add(c[:2])
                    unique_local.append(c)

            seen_nearby = set()
            unique_nearby = []
            for c in sorted(nearby_candidates, key=lambda x: x[2]):  # æŒ‰è·ç¦»æ’åº
                if c[:2] not in seen_local and c[:2] not in seen_nearby:  # é¿å…å’Œæœ¬åœ°é‡å¤
                    seen_nearby.add(c[:2])
                    unique_nearby.append(c)

            link_found = False
            searched_states = []

            # ç¬¬äºŒæ­¥ï¼šä¼˜å…ˆæŸ¥æœ¬åœ°å·ï¼ˆä¸¥æ ¼åŒ…å«çš„ï¼‰
            if unique_local:
                print(f"  ğŸ” é˜¶æ®µ1ï¼šæœç´¢æœ¬åœ°å· {[c[0] for c in unique_local]}...")
                for state, file_part, _ in unique_local:
                    if state not in index_dict:
                        print(f"    âš ï¸ æœ¬åœ°å·{state}æ— ç´¢å¼•æ–‡ä»¶")
                        continue

                    searched_states.append(f"{state}(æœ¬åœ°)")

                    # ç²¾ç¡®åŒ¹é…
                    for link in index_dict[state]:
                        if file_part in link:
                            down_dict_info[group_name][google_remote_file][link] = True
                            down_dict_info[group_name][google_remote_file]["__source__"] = f"{state}(æœ¬åœ°)"
                            link_found = True
                            print(f"    âœ… æœ¬åœ°å·{state}æ‰¾åˆ°: {link[:60]}...")
                            break

                    if link_found:
                        break

                    # # æ¨¡ç³ŠåŒ¹é…ï¼ˆé‚»å±…æ ¼å­ï¼‰
                    # if not link_found:
                    #     match = re.match(r'x(\d+)y(\d+)', file_part)
                    #     if match:
                    #         x, y = int(match.group(1)), int(match.group(2))
                    #         neighbors = [f"x{x - 1}y{y}", f"x{x + 1}y{y}", f"x{x}y{y - 1}", f"x{x}y{y + 1}"]
                    #         for neighbor in neighbors:
                    #             for link in index_dict[state]:
                    #                 if neighbor in link:
                    #                     print(f"    âš ï¸ æœ¬åœ°å·{state}è¿‘ä¼¼åŒ¹é…({neighbor}): {link[:60]}...")
                    #                     down_dict_info[group_name][google_remote_file][link] = True
                    #                     down_dict_info[group_name][google_remote_file][
                    #                         "__source__"] = f"{state}(æœ¬åœ°è¿‘ä¼¼)"
                    #                     down_dict_info[group_name][google_remote_file][
                    #                         "__note__"] = f"{file_part}->{neighbor}"
                    #                     link_found = True
                    #                     break
                    #             if link_found:
                    #                 break
                    #
                    # if link_found:
                    #     break

            # ç¬¬ä¸‰æ­¥ï¼šæœ¬åœ°å·æ²¡æ‰¾åˆ°ï¼Œæ‰©å¤§èŒƒå›´æŸ¥ä¸´è¿‘å·
            if not link_found and unique_nearby:
                print(f"  ğŸ” é˜¶æ®µ2ï¼šæœ¬åœ°å·æœªæ‰¾åˆ°ï¼Œæ‰©å¤§æœç´¢ä¸´è¿‘å· {[c[0] for c in unique_nearby]}...")

                for state, file_part, dist in unique_nearby:
                    if state not in index_dict:
                        continue

                    searched_states.append(f"{state}(ä¸´è¿‘,è·{dist:.2f}Â°)")
                    print(f"    æŸ¥ä¸´è¿‘å· {state} (è·ç¦»{dist:.2f}Â°)...")

                    # åŒæ ·å…ˆç²¾ç¡®åŒ¹é…
                    for link in index_dict[state]:
                        if file_part in link:
                            down_dict_info[group_name][google_remote_file][link] = True
                            down_dict_info[group_name][google_remote_file]["__source__"] = f"{state}(ä¸´è¿‘å·)"
                            down_dict_info[group_name][google_remote_file][
                                "__note__"] = f"æœ¬åœ°å·æœªæ‰¾åˆ°ï¼Œåœ¨{state}æ‰¾åˆ°(è·{dist:.2f}Â°)"
                            link_found = True
                            print(f"    âœ… ä¸´è¿‘å·{state}æ‰¾åˆ°: {link[:60]}...")
                            break

                    if link_found:
                        break

            # ç¬¬å››æ­¥ï¼šè®°å½•å¤±è´¥
            if not link_found:
                down_dict_info[group_name][google_remote_file]["__error__"] = "no_link_found"
                down_dict_info[group_name][google_remote_file]["__detail__"] = {
                    "æœ¬åœ°å·": [(c[0], c[1]) for c in unique_local],
                    # "ä¸´è¿‘å·": [(c[0], c[1], round(c[2], 4)) for c in unique_nearby],
                    "å·²æœç´¢": searched_states,
                    "tip": "æœ¬åœ°å·åŠ100kmå†…ä¸´è¿‘å·å‡æœªæ‰¾åˆ°è¯¥ç½‘æ ¼æ•°æ®"
                }
                print(f"    âŒ å¤±è´¥: æœ¬åœ°å·{[c[0] for c in unique_local]}åŠä¸´è¿‘å·å‡æœªæ‰¾åˆ°")

        except Exception as e:
            import traceback
            down_dict_info[group_name][google_remote_file]["__error__"] = "exception"
            down_dict_info[group_name][google_remote_file]["__detail__"] = str(e)

        # æ¯10ä¸ªä¿å­˜
        if len(down_dict_info[group_name]) % 10 == 0:
            with open(os.path.join(dam_usgs_dem_root_path, "DownloadInfo.json"), 'w') as f:
                json.dump(down_dict_info, f, ensure_ascii=False, indent=2)

    # æœ€ç»ˆä¿å­˜
    with open(os.path.join(dam_usgs_dem_root_path, "DownloadInfo.json"), 'w') as f:
        json.dump(down_dict_info, f, ensure_ascii=False, indent=2)

if __name__ == '__main__':

    index_dict = {}
    for file_name in os.listdir(dam_usgs_dem_index_root_path):
        key = file_name.split("_")[0]
        with open(os.path.join(dam_usgs_dem_index_root_path, file_name), "r", encoding="UTF-8") as f:
            for line in f:
                index_dict.setdefault(key, []).append(line.strip())

    usa_states_gdf = gpd.read_file(USA_States)

    # group_names = ["GeoDAR_v11_dams_of_USA_group1"]
    group_names = ["GeoDAR_v11_dams_of_USA_group1", "GeoDAR_v11_dams_of_USA_group10", "GeoDAR_v11_dams_of_USA_group11", "GeoDAR_v11_dams_of_USA_group12", "GeoDAR_v11_dams_of_USA_group13_1", "GeoDAR_v11_dams_of_USA_group13_2", "GeoDAR_v11_dams_of_USA_group14"]

    down_dict_info = {}

    for group_name in group_names:
        current_dam_google_remote_root_dir = os.path.join(dam_google_remote_root_path, group_name)
        output_dir = os.path.join(dam_usgs_dem_root_path, group_name)
        os.makedirs(output_dir, exist_ok=True)
        down_load_file(current_dam_google_remote_root_dir, index_dict, usa_states_gdf, group_name=group_name, down_dict_info=down_dict_info)

    with open(os.path.join(dam_usgs_dem_root_path, "DownloadInfo.json"), 'w', encoding='utf-8') as f:
        json.dump(down_dict_info, f, ensure_ascii=False, indent=2)
